<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chapters on DATA606 - Fall 2020</title>
    <link>/chapters/</link>
    <description>Recent content in Chapters on DATA606 - Fall 2020</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 29 Apr 2017 18:36:24 +0200</lastBuildDate><atom:link href="/chapters/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Chapter 1</title>
      <link>/chapters/chapter1/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter1/</guid>
      <description>Introduction to Data Learning Objectives  Identify the type of variables (e.g. numerical or categorical; discrete or continuous; ordered or not ordered). Identify the relationship between multiple variables (i.e. independent vs. dependent). Define variables that are not associated as independent. Be able to describe and identify the difference between observational and experimental studies. Distinguish between simple random, stratified, and cluster sampling, and recognize the benefits and drawbacks of choosing one sampling scheme over another.</description>
    </item>
    
    <item>
      <title>Chapter 2</title>
      <link>/chapters/chapter2/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter2/</guid>
      <description>Summarizing Data Learning Outcomes  Use appropriate visualizations for different types of data (e.g. histogram, barplot, scatterplot, boxplot, etc.). Use different measures of center and spread and be able to describe the robustness of different statistics. Describe the shape of distributions vis-a-vis histograms and boxplots. Create and intepret contingency and frequency tables (one- and two-way tables).  Supplemental Readings  OpenIntro Statistics slides ggplot2 - ggplot2 is an R package by Wickham that implements the grammer of graphics (Wilkinson, 2005) in R.</description>
    </item>
    
    <item>
      <title>Chapter 3</title>
      <link>/chapters/chapter3/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter3/</guid>
      <description>Probability Learning Outcomes  Define trial, outcome, and sample space. Define and describe the law of large numbers. Distinguish disjoint (also called mutually exclusive) and independent events. Use Venn diagrams to represent events and their probabilities. Describe probability distributions. Distinguish between marginal and conditional probabilities. Use tree diagrams and/or Bayes Theorem to calculate conditional probabilities and probabilities of intersection of non-independent events. The expected value of a discrete random variable is computed by adding each outcome weighted by its probability.</description>
    </item>
    
    <item>
      <title>Chapter 4</title>
      <link>/chapters/chapter4/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter4/</guid>
      <description>Distributions of Random Variables Learning Outcomes  Define the standardized (Z) score of a data point as the number of standard deviations it is away from the mean: $Z = \frac{x - \mu}{\sigma}$. Use the Z score  if the distribution is normal: to determine the percentile score of a data point (using technology or normal probability tables) regardless of the shape of the distribution: to assess whether or not the particular observation is considered to be unusual (more than 2 standard deviations away from the mean)   Depending on the shape of the distribution determine whether the median would have a negative, positive, or 0 Z score.</description>
    </item>
    
    <item>
      <title>Chapter 5</title>
      <link>/chapters/chapter5/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter5/</guid>
      <description>Foundations for Inference Learning Outcomes  Define sample statistic as a point estimate for a population parameter, for example, the sample proportion is used to estimate the population proportion, and note that point estimate and sample statistic are synonymous. Recognize that point estimates (such as the sample proportion) will vary from one sample to another, and define this variability as sampling variation. Calculate the sampling variability of the proportion, the standard error, as $SE = \sqrt{\frac{p(1-p)}{n}}$, where $p$ is the population proportion.</description>
    </item>
    
    <item>
      <title>Chapter 6</title>
      <link>/chapters/chapter6/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter6/</guid>
      <description>Inference for Categorical Data Learning Outcomes  Define population proportion $p$ (parameter) and sample proportion $\hat{p}$ (point estimate). Calculate the sampling variability of the proportion, the standard error, as [ SE = \sqrt{\frac{p(1-p)}{n}}, ] where $p$ is the population proportion.  Note that when the population proportion $p$ is not known (almost always), this can be estimated using the sample proportion, $SE = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$.   Recognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.</description>
    </item>
    
    <item>
      <title>Chapter 7</title>
      <link>/chapters/chapter7/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter7/</guid>
      <description>Inference for Numerical Data Learning Outcomes  Use the $t$-distribution for inference on a single mean, difference of paired (dependent) means, and difference of independent means. Explain why the $t$-distribution helps make up for the additional variability introduced by using $s$ (sample standard deviation) in calculation of the standard error, in place of $\sigma$ (population standard deviation). Describe how the $t$-distribution is different from the normal distribution, and what ?heavy tail?</description>
    </item>
    
    <item>
      <title>Chapter 8</title>
      <link>/chapters/chapter8/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter8/</guid>
      <description>Introduction to Linear Regression Learning Outcomes  Define the explanatory variable as the independent variable (predictor), and the response variable as the dependent variable (predicted). Plot the explanatory variable ($x$) on the x-axis and the response variable ($y$) on the y-axis, and fit a linear regression model $y = \beta_0 + \beta_1 x$ where $\beta_0$ is the intercept, and $\beta_1$ is the slope.  Note that the point estimates (estimated from observed data) for $\beta_0$ and $\beta_1$ are $b_0$ and $b_1$, respectively.</description>
    </item>
    
    <item>
      <title>Chapter 9</title>
      <link>/chapters/chapter9/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/chapter9/</guid>
      <description>Multiple and Logistic Regression Learning Outcomes  Define the multiple linear regression model as $$\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k$$ where there are $k$ predictors (explanatory variables). Interpret the estimate for the intercept ($b_0$) as the expected value of $y$ when all predictors are equal to 0, on average. Interpret the estimate for a slope (say $b_1$) as &amp;ldquo;All else held constant, for each unit increase in $x_1$, we would expect $y$ to increase/decrease on average by $b_1$.</description>
    </item>
    
    <item>
      <title>Bayesian</title>
      <link>/chapters/bayesian/</link>
      <pubDate>Sat, 29 Apr 2017 18:36:24 +0200</pubDate>
      
      <guid>/chapters/bayesian/</guid>
      <description>Bayesian Analysis Supplemental Readings  Chapter 17 of Learning Statistics with R (Navarro, version 0.6) Fitting a Model by Maximum Likelihood (Collier, 2013). Kruschke&amp;rsquo;s website for Doing Bayesian Data Analysis Kruschke&amp;rsquo;s blog Andrew Gelman&amp;rsquo;s blog - Posts about bayesian statistics  Videos Rasmus Bååth&amp;rsquo;s Introduction to Bayesian Data Analysis Video Series John Kruschke&amp;rsquo;s Video Series Bayesian Methods Interpret Data Better
Bayesian Estimation Supersedes the t Test
Precision is the goal</description>
    </item>
    
  </channel>
</rss>
